{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80b29cb",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "from numpy import arange\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1699e1",
   "metadata": {},
   "source": [
    "# Late fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493116ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration utility function\n",
    "def __get_all_accs(df, classes, d_types):\n",
    "    accs = {}\n",
    "    sum_of_accs = np.zeros(len(classes))\n",
    "    for d_type in d_types:\n",
    "        accs[d_type] = []\n",
    "        for i in range(len(classes)):\n",
    "            df_c = df.loc[(df['Real'] == classes[i]) & (df['Has '+d_type] != -1)]\n",
    "            acc = accuracy_score(df_c['Real'].values, df_c[d_type+' Pred'].values)\n",
    "            accs[d_type].append(acc)\n",
    "            sum_of_accs[i] += acc\n",
    "    return accs, sum_of_accs\n",
    "\n",
    "def __get_all_f1s(df, classes, d_types):\n",
    "    f1s = {}\n",
    "    sum_of_f1s = np.zeros(len(classes))\n",
    "    for d_type in d_types:\n",
    "        #accs[d_type] = []\n",
    "        df_c = df.loc[df['Has '+d_type] != -1]\n",
    "        f1_scores = f1_score(df_c['Real'].values, df_c[d_type+' Pred'].values, average=None)\n",
    "        f1s[d_type] = [f1_scores[0], f1_scores[1], f1_scores[2]]\n",
    "        for i in range(len(classes)):\n",
    "            sum_of_f1s[i] += f1_scores[i]\n",
    "    return f1s, sum_of_f1s\n",
    "\n",
    "def __get_all_f1s_total(df, classes, d_types):\n",
    "    f1s = {}\n",
    "    sum_of_f1s = 0\n",
    "    for d_type in d_types:\n",
    "        #accs[d_type] = []\n",
    "        df_c = df.loc[df['Has '+d_type] != -1]\n",
    "        f1 = f1_score(df_c['Real'].values, df_c[d_type+' Pred'].values, average='weighted')\n",
    "        f1s[d_type] = f1\n",
    "        sum_of_f1s += f1\n",
    "    return f1s, sum_of_f1s\n",
    "\n",
    "def __get_all_error_rates(df, classes, d_types):\n",
    "    error_rates = {}\n",
    "    sum_of_er = np.zeros(len(classes))\n",
    "    for d_type in d_types:\n",
    "        error_rates[d_type] = []\n",
    "        for i in range(len(classes)):\n",
    "            df_c = df.loc[(df['Real'] == classes[i]) & (df['Has '+d_type] != -1)]\n",
    "            acc = accuracy_score(df_c['Real'].values, df_c[d_type+' Pred'].values)\n",
    "            error_rate = (1-acc)*100\n",
    "            error_rates[d_type].append(error_rate)\n",
    "            sum_of_er[i] += error_rate\n",
    "    return error_rates, sum_of_er\n",
    "\n",
    "def get_probs_alphas_notgeneral(df: pd.DataFrame, classes: List[int], d_types: List[str]) -> List[float]:\n",
    "    \"\"\" Not general version of get alphas \"\"\"\n",
    "    def __get_all_accs(df, classes, d_types):\n",
    "        accs = {}\n",
    "        sum_of_accs = np.zeros(len(classes))\n",
    "        for d_type in d_types:\n",
    "            accs[d_type] = []\n",
    "            for i in range(len(classes)):\n",
    "                df_c = df.loc[(df['Real'] == classes[i]) & (df['Has '+d_type] != -1)]\n",
    "                acc = accuracy_score(df_c['Real'].values, df_c[d_type+' Pred'].values)\n",
    "                accs[d_type].append(acc)\n",
    "                sum_of_accs[i] += acc\n",
    "        return accs, sum_of_accs\n",
    "    \n",
    "    accs, sum_of_accs = __get_all_accs(df,classes,d_types)\n",
    "    alphas = {}\n",
    "    alphas[d_types[0]] = []\n",
    "    alphas[d_types[1]] = []\n",
    "    for i in range(len(classes)):\n",
    "        diff = abs(accs[d_types[0]][i] - accs[d_types[1]][i])\n",
    "        if accs[d_types[0]][i] > accs[d_types[1]][i]:\n",
    "            alphas[d_types[0]].append(0.5 + diff)\n",
    "            alphas[d_types[1]].append(0.5 - diff)  \n",
    "        else:\n",
    "            alphas[d_types[0]].append(0.5 - diff)\n",
    "            alphas[d_types[1]].append(0.5 + diff)\n",
    "\n",
    "    return alphas\n",
    "\n",
    "def get_probs_alphas(df: pd.DataFrame, classes: List[int], d_types: List[str]) -> List[float]:\n",
    "    accs, sum_of_accs = __get_all_f1s(df,classes,d_types)\n",
    "    alphas = {}\n",
    "    for d_type in d_types:\n",
    "        alphas[d_type] = []\n",
    "        for i in range(len(classes)):\n",
    "            alpha = accs[d_type][i]/sum_of_accs[i]\n",
    "            alphas[d_type].append(alpha)\n",
    "    return alphas\n",
    "\n",
    "def get_probs_alphas_total(df: pd.DataFrame, classes: List[int], d_types: List[str]) -> List[float]:\n",
    "    accs, sum_of_accs = __get_all_f1s_total(df,classes,d_types)\n",
    "    alphas = {}\n",
    "    for d_type in d_types:\n",
    "        alphas[d_type] = accs[d_type]/sum_of_accs\n",
    "    return alphas\n",
    "\n",
    "def optimize_alpha_train(df: pd.DataFrame, classes: List[int], d_types: List[str]) -> List[float]:\n",
    "    d_type1, dtype_2 = d_types[0], d_types[1]\n",
    "    best_alpha = [0,0]\n",
    "    best_acc = 0\n",
    "    for alpha in arange(0.1, 1, 0.05):\n",
    "        preds = []\n",
    "        df_c = df.loc[(df['Has ' + d_type1] != -1) & (df['Has ' + d_type2] != -1)]\n",
    "        real = df_c['Real'].values\n",
    "        luad_prob = (df_c[d_type1+' Prob LUAD']*alpha+(1-alpha)*df_c[d_type2+' Prob LUAD'])\n",
    "        hlt_prob = (df_c[d_type1+' Prob HLT']*alpha+(1-alpha)*df_c[d_type2+' Prob HLT'])\n",
    "        lusc_prob = (df_c[d_type1+' Prob LUSC']*alpha+(1-alpha)*df_c[d_type2+' Prob LUSC'])\n",
    "        for prob1, prob2, prob3 in zip(luad_prob, hlt_prob, lusc_prob):\n",
    "            pred = np.argmax([prob1, prob2, prob3])\n",
    "            preds.append(pred)\n",
    "        \n",
    "        acc = accuracy_score(real, preds)\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc\n",
    "            best_alpha[0] = alpha\n",
    "            best_alpha[1] = 1-alpha\n",
    "    \n",
    "    print(best_acc)\n",
    "    return best_alpha\n",
    "\n",
    "def integrate_probs(probs: List[float], class_index: int, alphas=None) -> float:\n",
    "    if alphas:\n",
    "        if type(alphas) == dict:\n",
    "            if class_index is not None:\n",
    "                weights_class = [x[1][class_index] for x in alphas.items()]\n",
    "            else:\n",
    "                weights_class = [x[1] for x in alphas.items()]\n",
    "        else:\n",
    "            weights_class = alphas\n",
    "        assert len(probs) == len(weights_class)\n",
    "        # use weights for each type of data\n",
    "        product = np.multiply(probs, weights_class)\n",
    "        return np.sum(product)\n",
    "    else:\n",
    "        # naive integration\n",
    "        product = probs.copy()\n",
    "    \n",
    "    # ommit zeros\n",
    "    idx = np.where(product != 0)[0]\n",
    "    product_clean = product[idx]\n",
    "    return np.sum(product_clean) / len(product_clean)\n",
    "\n",
    "def integrate_preds(preds: List[int]) -> int:\n",
    "    from collections import Counter\n",
    "    if len(preds) == 1:\n",
    "        return preds[0]\n",
    "    assert len(preds) > 2\n",
    "\n",
    "    return Counter(preds).most_common(1)[0][0]\n",
    "    \n",
    "def integration_model(data_types: List[str], datasets: List[str], name: str, path: str, \n",
    "                      fusion_type='probs', use_alphas=False, m_alphas=None) -> None:\n",
    "    for d in datasets:\n",
    "        writer = pd.ExcelWriter(path+'data_integration_model_'+d+'_'+fusion_type+'_'+name+'.xlsx', engine='openpyxl')\n",
    "        data = pd.read_excel('early_integration/RNA6_CNV12_'+d+'_new.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "        if use_alphas:\n",
    "            print('Getting alphas from training set...')\n",
    "            data_train = pd.read_excel('early_integration/RNA6_CNV12_train_new.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "            splits_alphas = {}\n",
    "            for df_name, df in data_train.items():\n",
    "                df = df.loc[(df['Has '+data_types[0]] != -1) & (df['Has '+data_types[1]] != -1)]\n",
    "                alphas = get_probs_alphas(df, [0,1,2], data_types)\n",
    "                splits_alphas[df_name] = alphas\n",
    "                print(alphas)\n",
    "\n",
    "        for df_name, df in data.items():\n",
    "            integration_probs = {\n",
    "            'LUAD': [],\n",
    "            'HLT': [],\n",
    "            'LUSC': []\n",
    "            }\n",
    "            integration_preds = []\n",
    "            for _, row in tqdm(df.iterrows()):\n",
    "                local_probs = {\n",
    "                    'LUAD': [],\n",
    "                    'HLT': [],\n",
    "                    'LUSC': []\n",
    "                }\n",
    "                local_preds = []\n",
    "                for d_type in data_types:\n",
    "                    if row['Has ' + d_type] != -1:\n",
    "                        luad = row[d_type+ ' Prob LUAD']\n",
    "                        hlt = row[d_type+ ' Prob HLT']\n",
    "                        lusc = row[d_type+ ' Prob LUSC']\n",
    "                        luad_new = luad / (luad + hlt +lusc)\n",
    "                        hlt_new = hlt / (luad + hlt +lusc)\n",
    "                        lusc_new = lusc / (luad + hlt +lusc)\n",
    "                        local_probs['LUAD'].append(luad_new)\n",
    "                        local_probs['HLT'].append(hlt_new)\n",
    "                        local_probs['LUSC'].append(lusc_new)\n",
    "                        local_preds.append(row[d_type + ' Pred'])\n",
    "                    elif use_alphas:\n",
    "                        local_probs['LUAD'].append(0)\n",
    "                        local_probs['HLT'].append(0)\n",
    "                        local_probs['LUSC'].append(0)\n",
    "                        \n",
    "                if fusion_type == 'probs':\n",
    "                    if use_alphas:\n",
    "                        if m_alphas:\n",
    "                            alphas_manual = m_alphas\n",
    "                            luad_prob = integrate_probs(local_probs['LUAD'], 0, alphas_manual)\n",
    "                            hlt_prob = integrate_probs(local_probs['HLT'], 1, alphas_manual)\n",
    "                            lusc_prob = integrate_probs(local_probs['LUSC'], 2, alphas_manual)\n",
    "                        else:\n",
    "                            luad_prob_new = integrate_probs(local_probs['LUAD'], 0, splits_alphas[df_name])\n",
    "                            hlt_prob_new = integrate_probs(local_probs['HLT'], 1, splits_alphas[df_name])\n",
    "                            lusc_prob_new = integrate_probs(local_probs['LUSC'], 2, splits_alphas[df_name])\n",
    "                            luad_prob = luad_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                            lusc_prob = lusc_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                            hlt_prob = hlt_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                    else:\n",
    "                        luad_prob = integrate_probs(local_probs['LUAD'])\n",
    "                        hlt_prob = integrate_probs(local_probs['HLT'])\n",
    "                        lusc_prob = integrate_probs(local_probs['LUSC'])\n",
    "                    integration_probs['LUAD'].append(luad_prob)\n",
    "                    integration_probs['HLT'].append(hlt_prob)\n",
    "                    integration_probs['LUSC'].append(lusc_prob)\n",
    "\n",
    "                    pred = np.argmax([luad_prob,hlt_prob,lusc_prob], axis=0)\n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "                elif fusion_type == 'preds':\n",
    "                    if len(local_preds) == 2:\n",
    "                        # if there are only two predictions, we need to fuse the probabilities\n",
    "                        luad_prob = integrate_probs(local_probs['LUAD'])\n",
    "                        hlt_prob = integrate_probs(local_probs['HLT'])\n",
    "                        lusc_prob = integrate_probs(local_probs['LUSC'])\n",
    "                        \n",
    "                        integration_probs['LUAD'].append(luad_prob)\n",
    "                        integration_probs['HLT'].append(hlt_prob)\n",
    "                        integration_probs['LUSC'].append(lusc_prob)\n",
    "                        \n",
    "                        pred = np.argmax([luad_prob,hlt_prob,lusc_prob], axis=0)\n",
    "                        \n",
    "                    else:\n",
    "                        pred = integrate_preds(local_preds)\n",
    "                        if pred == 0:\n",
    "                            luad_prob = 1\n",
    "                            hlt_prob = 0\n",
    "                            lusc_prob = 0\n",
    "                        elif pred == 1:\n",
    "                            luad_prob = 0\n",
    "                            hlt_prob = 1\n",
    "                            lusc_prob = 0\n",
    "                        else:\n",
    "                            luad_prob = 0\n",
    "                            hlt_prob = 0\n",
    "                            lusc_prob = 1\n",
    "\n",
    "                        integration_probs['LUAD'].append(luad_prob)\n",
    "                        integration_probs['HLT'].append(hlt_prob)\n",
    "                        integration_probs['LUSC'].append(lusc_prob)\n",
    "                    \n",
    "                \n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "            \n",
    "            for cls in integration_probs.keys():\n",
    "                df['Integration Prob '+ cls] = integration_probs[cls]\n",
    "\n",
    "            df['Integration Pred'] = integration_preds\n",
    "\n",
    "            # save to sheet\n",
    "            df.to_excel(writer, sheet_name='split_'+str(df_name), index=False)\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162df054",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_model(data_types=[\"RNA\", \"CNV\"],\n",
    "                  datasets=['test','train'], name=\"RNA-CNV-alphas-XGBOOST_both\",\n",
    "                  path='',\n",
    "                  fusion_type='probs', use_alphas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01953439",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "\n",
    "data_model = pd.read_excel('data_integration_model_test_probs_RNA-CNV-alphas-XGBOOST_both_total.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "d_type1 = 'RNA'\n",
    "d_type2 = 'CNV'\n",
    "name = d_type1 + d_type2\n",
    "accs[name] = []\n",
    "f1_scores[name] = []\n",
    "aucs[name] = []\n",
    "\n",
    "writer = pd.ExcelWriter('excel_results/RNA-CNV-Integration-late-fusion-XGBOOST_both.xlsx', engine='openpyxl')\n",
    "for df_name, df in data_model.items():\n",
    "    # take those where the two sources has data\n",
    "    df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1)]\n",
    "\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "    preds = df_only['Integration Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    print(Counter(real))\n",
    "    print('Acc: {}; F1; {}'.format(acc, f1))\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    accs[name].append(acc)\n",
    "    f1_scores[name].append(f1)\n",
    "    columns_save = ['Case IDs', 'Has RNA', 'Has CNV', 'RNA Prob LUAD', 'RNA Prob HLT',\n",
    "                    'RNA Prob LUSC', 'RNA Pred', 'CNV Prob LUAD', 'CNV Prob HLT',\n",
    "                    'CNV Prob LUSC', 'CNV Pred', 'Integration Prob LUAD', 'Integration Prob HLT',\n",
    "                    'Integration Prob LUSC', 'Integration Pred', 'Real']\n",
    "    new_df = df_only[columns_save]\n",
    "    new_df.to_excel(writer, sheet_name='split_'+str(df_name), index=False)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
